<!DOCTYPE html><html lang="en" class="antialiased break-words"> <head><!-- High Priority Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Standards in Geo-AI | Horizournal</title><meta name="generator" content="Astro v4.3.5"><!-- Fonts --><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Figtree:ital,wght@0,400..700;1,400..700&display=swap"><link href="https://fonts.googleapis.com/css2?family=Figtree:ital,wght@0,400..700;1,400..700&display=swap" rel="stylesheet"><!-- Low Priority Global Metadata --><link rel="icon" type="image/svg+xml" href="/icon.png"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" href="/rss.xml" title="RSS"><!-- Page Metadata --><link rel="canonical" href="https://kshitijrajsharma.github.io/blog/standards-in-geoai/"><meta name="description" content="STAC-MLM, OGC TrainingDML-AI, and Related Initiatives"><!-- Open Graph / Facebook --><meta property="og:type" content="article"><meta property="og:url" content="https://kshitijrajsharma.github.io/blog/standards-in-geoai/"><meta property="og:title" content="Standards in Geo-AI | Horizournal"><meta property="og:description" content="STAC-MLM, OGC TrainingDML-AI, and Related Initiatives"><meta property="og:image" content="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeos8LBuB72Bzsf8aNZUVMwlgseWPmTJiZ119U1gFW90iikqeZnkbs85pRIPvwVMGpmQmT8CpCFBjFpywdrLzYZBqn1az99J8Vf_USWAPOompc4b9mh5Tipovw3dMGq5HRTKiBW?key=e0PKQH475XwTzK6KsQ_zXI2w"><meta property="og:image:alt" content="STAC-LLM"><!-- X/Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://kshitijrajsharma.github.io/blog/standards-in-geoai/"><meta property="twitter:title" content="Standards in Geo-AI | Horizournal"><meta property="twitter:description" content="STAC-MLM, OGC TrainingDML-AI, and Related Initiatives"><meta property="twitter:image" content="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeos8LBuB72Bzsf8aNZUVMwlgseWPmTJiZ119U1gFW90iikqeZnkbs85pRIPvwVMGpmQmT8CpCFBjFpywdrLzYZBqn1az99J8Vf_USWAPOompc4b9mh5Tipovw3dMGq5HRTKiBW?key=e0PKQH475XwTzK6KsQ_zXI2w"><meta name="twitter:image:alt" content="STAC-LLM"><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><link rel="stylesheet" href="/_astro/_slug_.ZjW95MmL.css" /><script type="module" src="/_astro/hoisted.2LRzaFmH.js"></script></head> <body class="bg-white text-slate-700"> <div class="flex flex-col min-h-screen"> <header class="px-4 py-4 flex gap-6 items-center justify-between sm:px-8 sm:py-6 bg-slate-50 border-b border-slate-100" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2> <img class="max-h-16" src="/logo.png" alt="Horizournal logo" data-astro-cid-3ef6ksr2> </a> <nav class="relative" data-astro-cid-3ef6ksr2> <button class="menu-toggle w-10 h-10 inline-flex items-center justify-center bg-primary text-white rounded-full relative z-20 transition-shadow duration-300 hover:shadow-button" aria-label="Open Menu" aria-expanded="false" aria-controls="menu-items" data-astro-cid-3ef6ksr2> <span class="menu-toggle-icon w-1 h-1 relative bg-white rounded-full" data-astro-cid-3ef6ksr2></span> </button> <ul id="menu-items" class="menu absolute -top-3 -right-3 px-8 pt-16 pb-10 bg-white/75 border border-white/40 z-10 sm:px-12 sm:pt-20 sm:pb-16" data-astro-cid-3ef6ksr2> <li class="mb-1 border-b border-slate-200/70" data-astro-cid-3ef6ksr2> <a class="block pr-6 py-1 text-lg text-slate-700 font-bold sm:text-xl relative transition duration-300 hover:text-slate-500" href="/" data-astro-cid-3ef6ksr2> Home </a> </li><li class="mb-1 border-b border-slate-200/70" data-astro-cid-3ef6ksr2> <a class="block pr-6 py-1 text-lg text-slate-700 font-bold sm:text-xl relative transition duration-300 hover:text-slate-500" href="/blog" data-astro-cid-3ef6ksr2> Blog </a> </li><li class="mb-1 border-b border-slate-200/70" data-astro-cid-3ef6ksr2> <a class="block pr-6 py-1 text-lg text-slate-700 font-bold sm:text-xl relative transition duration-300 hover:text-slate-500" href="/about" data-astro-cid-3ef6ksr2> About </a> </li><li class="mb-1 border-b border-slate-200/70" data-astro-cid-3ef6ksr2> <a class="block pr-6 py-1 text-lg text-slate-700 font-bold sm:text-xl relative transition duration-300 hover:text-slate-500" href="/contact" data-astro-cid-3ef6ksr2> Contact </a> </li> </ul> </nav> </header>    <main class="grow px-4 py-12 sm:px-8 sm:py-16"> <article class="max-w-6xl mx-auto mb-12 sm:mb-16"> <header class="max-w-3xl mx-auto mb-6 sm:mb-8"> <div class="mb-3 text-sm uppercase tracking-wider text-slate-500">  <time datetime="2025-04-25T22:00:00.000Z"> April 26, 2025 </time>  </div> <h1 class="text-3xl font-bold text-slate-900 sm:text-4xl md:text-5xl">Standards in Geo-AI: STAC-MLM, OGC TrainingDML-AI, and Related Initiatives</h1> </header> <figure class="mx-auto my-6 sm:my-8 lg:my-12"> <img class="w-full rounded-lg" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeos8LBuB72Bzsf8aNZUVMwlgseWPmTJiZ119U1gFW90iikqeZnkbs85pRIPvwVMGpmQmT8CpCFBjFpywdrLzYZBqn1az99J8Vf_USWAPOompc4b9mh5Tipovw3dMGq5HRTKiBW?key=e0PKQH475XwTzK6KsQ_zXI2w" loading="lazy" decoding="async" alt="STAC-LLM">  </figure> <div class="max-w-3xl mx-auto"> <div class="prose prose-slate sm:prose-lg max-w-none"> <h1 id="introduction">Introduction</h1>
<p>Geospatial AI has rapidly grown, with applications ranging from
land-cover mapping to disaster damage assessment. However, a historical
lack of standards for organizing training data and models has led to
fragmented practices (<a href="#ref-copernicus2023">Yue 2023</a>). In recent years,
multiple initiatives have emerged to improve the FAIR (Findable,
Accessible, Interoperable, Reusable) principles in GeoAI by
standardizing how datasets and models are described and shared. This
review examines key efforts, including the STAC-MLM extension for model
metadata, the OGC TrainingDML-AI standard for training data, open-source
frameworks like Raster Vision and TerraTorch, and commercial approaches
such as Esri’s GeoAI toolbox. We compare their scope, design, and
adoption, and discuss trends and humanitarian use cases.</p>
<h1 id="stac-mlm-spatiotemporal-asset-catalog--machine-learning-model-extension">STAC-MLM: SpatioTemporal Asset Catalog – Machine Learning Model Extension</h1>
<p><a href="https://github.com/stac-extensions/mlm">STAC-MLM</a> is a community-driven
extension to the SpatioTemporal Asset Catalog (STAC) specification,
designed to catalog and describe machine learning models that use
geospatial data (<a href="#ref-wherobots2023">Wherobots 2023</a>). Developed by
organizations such as CRIM, Wherobots, Terradue, Radiant Earth, and
NRCan, and presented at the 2024 ACM SIGSPATIAL conference
(<a href="#ref-charette2024">Charette-Migneault, Bédard, and Vincent 2024</a>),
STAC-MLM aims to make models discoverable alongside datasets and to
capture all metadata needed for reuse or deployment. However, this is
not the first one that tries to use STAC for machine learning it is more
of successor of the extension which was started at Radiant Earth on
October 4th, 2021. It was possibly the first STAC extension dedicated to
describing machine learning models as per this
<a href="https://github.com/stac-extensions/ml-model/pull/19/files">PR</a></p>
<figure>
<img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeos8LBuB72Bzsf8aNZUVMwlgseWPmTJiZ119U1gFW90iikqeZnkbs85pRIPvwVMGpmQmT8CpCFBjFpywdrLzYZBqn1az99J8Vf_USWAPOompc4b9mh5Tipovw3dMGq5HRTKiBW?key=e0PKQH475XwTzK6KsQ_zXI2w" alt="Overview of STAC-MLM">
<figcaption aria-hidden="true">Overview of STAC-MLM</figcaption>
</figure>
<p>Key features of STAC-MLM include:</p>
<ul>
<li><strong>Geospatial context</strong>: Spatial and temporal domain of the model,
clarifying where and when it is applicable.</li>
<li><strong>Input data specifications</strong>: Required sensor bands or input types
and any preprocessing steps (e.g., rescaling, normalization) needed
(<a href="#ref-wherobots2023">Wherobots 2023</a>).</li>
<li><strong>Output description</strong>: The model’s output shape, type, and semantic
meaning (e.g., class labels for segmentation).</li>
<li><strong>Runtime requirements</strong>: An optional description of required runtime
environments, frameworks, or hardware to ensure reproducibility.</li>
<li><strong>Provenance and references</strong>: Links to training details, scientific
papers, or dataset sources.</li>
</ul>
<p>STAC-MLM reuses STAC fields where possible and enables models to be
represented as STAC Items or Collections, making them searchable via
standard STAC APIs. By structuring model metadata consistently, it
ensures that models become first-class, discoverable assets (<a href="#ref-wherobots2023">Wherobots
2023</a>).</p>
<p>The motivation behind STAC-MLM arose from inconsistent documentation
practices, where data scientists repurposed general-purpose model cards
without standardizing geospatial-specific metadata (<a href="#ref-wherobots2023">Wherobots
2023</a>). By introducing a common schema, STAC-MLM
enables model portability and consistency, allowing a model published by
one group to be understood and reused by others more easily.</p>
<p><strong>Adoption</strong>: Despite being relatively new, STAC-MLM has seen early
adoption. Wherobots’ “Raster Inference” platform uses STAC-MLM for model
imports, allowing users to bring their models with a JSON descriptor.
Radiant Earth’s MLHub and Terradue workflows have also integrated
STAC-MLM, further demonstrating its practical utility (<a href="#ref-wherobots2023">Wherobots
2023</a>; <a href="#ref-radiant2021">Radiant Earth Foundation
2021</a>). I particularly like this form builder for
validating STAC for ML : <a href="https://mlm-form.vercel.app/">ML Model stac
validator</a> You can find schema reference
<a href="https://stac-extensions.github.io/mlm/v1.4.0/schema.json">here</a> .</p>
<h1 id="ogc-trainingdml-ai-standardizing-geospatial-training-data">OGC TrainingDML-AI: Standardizing Geospatial Training Data</h1>
<p>While STAC-MLM focuses on model metadata, the <a href="https://docs.ogc.org/is/23-008r3/23-008r3.html">OGC
TrainingDML-AI</a> standard
addresses the organization of training datasets. Released by the Open
Geospatial Consortium in 2023–2024, TrainingDML-AI defines a conceptual
model and encoding methods (JSON, XML) for documenting geospatial ML
training datasets (<a href="#ref-ogc2023">Open Geospatial Consortium 2023</a>).</p>
<p>Key conceptual entities in TrainingDML-AI include:</p>
<ul>
<li><strong>AI_TrainingDataset</strong>: The collection of training samples (e.g., a
dataset for land cover classification).</li>
<li><strong>AI_TrainingData</strong>: A single training sample, such as an image and
its corresponding labels.</li>
<li><strong>AI_Task</strong>: The ML task type (e.g., classification, detection,
segmentation).</li>
<li><strong>AI_Label</strong>: Semantic description of labels, supporting scene-level,
object-level, and pixel-level labels.</li>
<li><strong>AI_Labeling</strong>: Documentation of labeling procedures, provenance, and
responsible entities, aligned with W3C PROV.</li>
<li><strong>DataQuality</strong>: Metadata describing dataset quality and
uncertainties, following ISO 19157 standards.</li>
<li><strong>AI_TDChangeset</strong>: Versioning metadata to track updates or
corrections between dataset releases.</li>
</ul>
<figure>
<img src="https://hackmd.io/_uploads/rkkqWf91xl.png" style="width:60.0%" alt="TrainingDML-AI module overview">
<figcaption aria-hidden="true">TrainingDML-AI module
overview</figcaption>
</figure>
<p>TrainingDML-AI cleanly separates general metadata from domain-specific
extensions, such as Earth Observation fields, supporting a wide range of
AI domains while maintaining flexibility (<a href="#ref-copernicus2023">Yue
2023</a>).</p>
<p>The standard also defines JSON encoding formats to allow
machine-readable and web-compatible descriptions, making it easier to
publish, discover, and validate geospatial training datasets.</p>
<p><strong>Comparison to STAC Label Extension</strong>: Before TrainingDML-AI, Radiant
MLHub primarily used the STAC Label Extension to package training
datasets. While effective, the STAC Label Extension focused primarily on
imagery and label connections, lacking structured provenance, data
quality, and versioning metadata. TrainingDML-AI expands this scope,
offering a richer, more extensible model for dataset description (<a href="#ref-copernicus2023">Yue
2023</a>).</p>
<h1 id="radiant-mlhub-currently-source-cooperative-and-prior-community-efforts">Radiant MLHub (Currently Source Cooperative) and Prior Community Efforts</h1>
<p>Prior to the establishment of STAC-MLM and TrainingDML-AI, the
geospatial community had already recognized the need for standardization
around training datasets. Radiant Earth Foundation launched Radiant
MLHub in 2019 as an open registry for geospatial ML datasets (<a href="#ref-radiant2019">Radiant
Earth Foundation 2019</a>). Radiant MLHub uses a
STAC-compliant catalog and the STAC Label Extension to store imagery and
corresponding labels in a consistent, machine-readable format. However I
couldn’t find the working link of radiant MLHUB at the moment. Only this
<a href="https://medium.com/radiant-earth-insights/accessing-and-downloading-training-data-on-the-radiant-mlhub-api-f04dc635592f">Blog</a>
, As per this
<a href="https://radiant.earth/blog/2023/05/radiant-earth-announces-new-initiatives-to-accelerate-sharing-of-earth-science-data/">Article</a>
As of October 2023, all content previously available on Radiant MLHub
has been migrated to <a href="https://source.coop/">Source Cooperative</a>, and
access to Radiant MLHub has been discontinued. Source Cooperative
continues to be developed and supported by Radiant Earth, with the goal
of making Earth science data more accessible and easier to use.</p>
<p>Each dataset on MLHub is organized as a STAC Collection of imagery Items
and Label Items, with standardized metadata describing label classes,
geospatial information, and dataset splits (train/validation). This
early effort pioneered the FAIR publication of training datasets,
allowing users to programmatically search and access resources through
standard APIs (<a href="#ref-radiant2021">Radiant Earth Foundation 2021</a>).</p>
<p>In addition to datasets, Radiant Earth expanded MLHub to include
pretrained geospatial models by late 2021. These models were cataloged
using an early version of the STAC ML Model Extension , effectively a
precursor to STAC-MLM , linking models to their corresponding training
datasets (<a href="#ref-radiant2021">Radiant Earth Foundation 2021</a>). For
example, Radiant Earth cataloged a tropical storm wind-speed estimation
model alongside its training data, creating a clear lineage between data
and models. This approach made it possible for researchers to query
models based on task, region, or input data type, significantly
improving model discoverability.</p>
<p>Other early community initiatives also contributed to standardization
efforts. The OGC Testbed-18 project in 2022 examined metadata best
practices for training datasets and recommended paths toward
standardization (<a href="#ref-ogc2023">Open Geospatial Consortium 2023</a>).
Meanwhile, prototypes like the Deep Learning Metadata (DLM) proposal and
an early ML Model STAC extension by groups such as Azavea and USGS laid
important groundwork, although they lacked broad adoption. The design of
the current STAC-MLM unified these earlier ideas into a more
comprehensive and sustainable community standard.</p>
<p>In summary, early efforts like Radiant MLHub demonstrated that existing
geospatial data standards (e.g., STAC) could be adapted to handle
machine learning needs. These experiences informed the development of
newer, formal standards like TrainingDML-AI and STAC-MLM, ensuring
compatibility with the broader geospatial community’s tools and
practices.</p>
<h1 id="raster-vision-open-source-geoai-pipeline">Raster Vision: Open-Source GeoAI Pipeline</h1>
<p>While standards like STAC-MLM and TrainingDML-AI address data and model
documentation, open-source frameworks such as <a href="https://rastervision.io/">Raster
Vision</a> focus on operationalizing end-to-end
machine learning pipelines for geospatial data. Initially developed by
Azavea and now community-maintained, Raster Vision provides a
configurable system for training and deploying models on overhead
imagery (<a href="#ref-rastervision2020">Azavea 2020</a>).</p>
<figure>
<img src="https://hackmd.io/_uploads/B1LJMzcJle.png" alt="Raster Vision Pipeline">
<figcaption aria-hidden="true">Raster Vision Pipeline</figcaption>
</figure>
<p>Raster Vision supports common geospatial ML tasks, including:</p>
<ul>
<li><strong>Chip Classification</strong>: Classifying small image patches (useful for
land-cover classification).</li>
<li><strong>Object Detection</strong>: Detecting and localizing objects using bounding
boxes.</li>
<li><strong>Semantic Segmentation</strong>: Per-pixel classification to delineate
features like buildings, roads, and vegetation.</li>
</ul>
<p>The framework handles geospatial-specific challenges such as very large
image sizes, multiple spectral bands, and diverse coordinate systems.
Raster Vision can ingest standard GIS data formats (e.g., GeoJSON,
shapefiles) and satellite imagery (e.g., GeoTIFFs). It is flexible with
input data formats, supporting labeling schemas like COCO JSON and
Pascal VOC XML.</p>
<p>While Raster Vision does not impose a new data standard, it aligns well
with community practices. For example, it can ingest imagery and labels
organized using STAC catalogs. The developers have also expressed
interest in deeper STAC integration, where data discovery could happen
dynamically through STAC APIs (<a href="#ref-rastervision2020">Azavea 2020</a>).</p>
<p>Models trained with Raster Vision are bundled with configuration files
and sample outputs, creating a semi-standardized model package. Although
<strong>Raster Vision does not yet natively export STAC-MLM metadata</strong>, users
could manually create an MLM JSON descriptor based on the training
configuration, facilitating broader sharing and reuse.</p>
<p>In practice, Raster Vision has been widely adopted in the humanitarian
and development sectors for tasks such as crop mapping in Africa and
post-disaster building detection. Its emphasis on reproducible pipelines
and flexible data handling makes it a key component in the emerging
GeoAI ecosystem.</p>
<h1 id="terratorch-and-geospatial-foundation-models">TerraTorch and Geospatial Foundation Models</h1>
<p>The rise of geospatial foundation models ; very large models pre-trained
on broad Earth observation data has brought new tools and challenges to
the GeoAI field. <a href="https://ibm.github.io/terratorch/stable/">TerraTorch</a>,
developed by IBM Research and collaborators in 2023–2024, is an
open-source toolkit designed to fine-tune and benchmark these foundation
models (<a href="#ref-terratorch2024">Kumar, Suresh, and Srivastava 2024</a>).</p>
<p>Built on PyTorch Lightning, TerraTorch provides modular components
specialized for satellite, weather, and climate data. It offers
domain-specific data modules for handling multi-band imagery and
time-series datasets, along with preconfigured tasks for classification,
regression, and segmentation.</p>
<p>Key features of TerraTorch include:</p>
<ul>
<li>A <strong>model factory</strong> enabling users to swap pre-trained backbone
encoders and task-specific decoder heads without coding.</li>
<li><strong>No-code fine-tuning</strong> workflows using YAML/JSON configurations.</li>
<li><strong>Automated hyperparameter tuning</strong> through an “Iterate” extension to
optimize training runs.</li>
<li><strong>Integration with GEO-Bench</strong>, a benchmark suite offering
standardized geospatial evaluation tasks for foundation models.</li>
</ul>
<figure>
<img src="https://hackmd.io/_uploads/B1AxbGcJxx.png" style="width:60.0%" alt="TerraTorch Pipeline">
<figcaption aria-hidden="true">TerraTorch Pipeline</figcaption>
</figure>
<p>TerraTorch enables users to quickly adapt large pre-trained models, such
as Prithvi-EO or Clay, to specific applications like flood mapping or
crop classification. Although it does not define a new metadata
standard, TerraTorch consumes data via standard geospatial loaders like
TorchGeo, which can read STAC catalogs and related formats.</p>
<p>By integrating standardized evaluation suites like GEO-Bench, TerraTorch
supports reproducible benchmarking of foundation models. As foundation
models become more common in Earth observation, frameworks like
TerraTorch will likely play a central role in operationalizing them for
humanitarian and climate applications.</p>
<h1 id="commercial-geoai-platforms-esris-approach">Commercial GeoAI Platforms: Esri’s Approach</h1>
<p>Commercial GIS platforms have also embraced GeoAI integration, albeit
with different approaches to standardization. Esri’s ArcGIS Pro includes
a <a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/geoai/an-overview-of-the-geoai-toolbox.htm">GeoAI
toolbox</a>
that provides tools for training and applying AI models on geospatial
data (<a href="#ref-esri2024">Esri 2024</a>).</p>
<p>The GeoAI toolbox supports:</p>
<ul>
<li>Training regression and classification models on spatial tabular data.</li>
<li>Object detection and pixel classification on imagery.</li>
<li>Natural language processing tools for text geolocation and time-series
forecasting.</li>
</ul>
<p>Rather than creating new data standards, Esri ensures compatibility with
widely used machine learning formats such as COCO, Pascal VOC, and
KITTI. These formats allow data interoperability between ArcGIS and
popular open-source frameworks like PyTorch and TensorFlow.</p>
<p>For model metadata, Esri defines the Esri Model Definition (.emd)
format, a JSON structure describing input channels, class names, model
architecture, and inference parameters. EMD files are bundled with model
weights into Deep Learning Packages (.dlpk) for deployment within
ArcGIS.</p>
<p>While EMD serves a similar purpose to STAC-MLM—capturing model
metadata—it is Esri-specific and oriented toward its ecosystem. STAC-MLM
remains platform-agnostic, intended for broader discovery and reuse
across systems.</p>
<p>Esri’s GeoAI tools emphasize user-friendliness, allowing non-programmers
to train and deploy models within familiar GIS workflows. However, they
also create some siloing: models and metadata created in ArcGIS may
require conversion before being used in fully open ecosystems.</p>
<h1 id="comparison-of-approaches">Comparison of Approaches</h1>
<p>Different initiatives in GeoAI standardization target different parts of
the machine learning lifecycle:</p>
<ul>
<li><strong>Scope</strong>: STAC-MLM standardizes model metadata, while OGC
TrainingDML-AI standardizes training data metadata. Raster Vision and
TerraTorch focus on operationalizing workflows. Esri GeoAI offers
end-to-end tools within its proprietary platform.</li>
<li><strong>Type</strong>: STAC-MLM and TrainingDML-AI are formal standards. Radiant
MLHub and GEO-Bench implement standards. Raster Vision and TerraTorch
are open-source frameworks. Esri GeoAI is a proprietary
implementation.</li>
<li><strong>Community involvement</strong>: STAC-MLM and TrainingDML-AI were developed
through open collaboration involving multiple stakeholders. Raster
Vision and TerraTorch are maintained by open communities or
open-source foundations. Esri’s efforts are developed internally with
user input but are closed source.</li>
<li><strong>Interoperability</strong>: STAC-MLM and TrainingDML-AI are designed for
interoperability across platforms and organizations. Raster Vision and
TerraTorch increasingly support standardized formats like STAC. Esri
focuses on in-ecosystem workflows but allows for import/export in
common formats.</li>
<li><strong>Metadata richness</strong>: TrainingDML-AI includes detailed provenance and
data quality metadata. STAC-MLM captures model input requirements,
output semantics, and runtime environments. Earlier practices often
lacked this level of documentation.</li>
<li><strong>FAIR principles</strong>: All reviewed initiatives emphasize improving
Findability, Accessibility, Interoperability, and Reusability,
although with varying degrees of emphasis and maturity.</li>
</ul>
<p>Overall, STAC-MLM and TrainingDML-AI complement each other, providing
metadata coverage across the data-model pipeline. Raster Vision and
TerraTorch operationalize GeoAI tasks, while Esri focuses on
accessibility within a commercial GIS environment.</p>
<h1 id="trends-in-geoai-standardization">Trends in GeoAI Standardization</h1>
<p>A major trend in the GeoAI landscape is the convergence on STAC as a
backbone for organizing both geospatial datasets and machine learning
models. Many initiatives, including Radiant MLHub, STAC-MLM, Raster
Vision (roadmap), and AWS data catalogs, are either building on or
planning support for STAC-based structures (<a href="#ref-charette2024">Charette-Migneault, Bédard,
and Vincent 2024</a>; <a href="#ref-radiant2021">Radiant Earth Foundation
2021</a>). This convergence enables more seamless
discovery: an analyst could potentially query a single STAC API to find
both input data and pretrained models for a given task.</p>
<p>Another trend is the formalization of community practices into industry
standards. The involvement of the Open Geospatial Consortium (OGC) in
formalizing TrainingDML-AI demonstrates that FAIR dataset practices are
maturing into internationally recognized protocols. As TrainingDML-AI
becomes more widely adopted, tools and repositories are likely to
incorporate automated validation, metadata conversion, and training
workflows based on these standards (<a href="#ref-ogc2023">Open Geospatial Consortium
2023</a>).</p>
<p>Open-source tools are also evolving to integrate these standards. PySTAC
libraries now support STAC-MLM, and emerging tools like pyTDML are aimed
at supporting TrainingDML-AI datasets. Raster Vision and TerraTorch are
aligning with open data access via STAC APIs and TorchGeo modules,
respectively.</p>
<p>Finally, the rise of geospatial foundation models, such as IBM-NASA’s
Prithvi-EO and other self-supervised Earth observation models, is
driving a need for standardized benchmarking and metadata. GEO-Bench
provides a common evaluation suite, while TerraTorch operationalizes
benchmarking workflows. This signals a broader maturing of GeoAI, moving
from isolated experiments toward a reproducible, scalable discipline.</p>
<p>As GeoAI models move from research to production, <strong>deployment and
containerization</strong> strategies have become critical for standardization.
One prominent trend is using orchestration frameworks
(<a href="https://flyte.org/">Flyte</a>, <a href="https://www.kubeflow.org/">Kubeflow</a>,
<a href="https://www.zenml.io/">ZenML</a>, etc.) on Kubernetes clusters to run
geospatial ML pipelines in a repeatable way. I need to do further
research on this and perhaps include those topics in next blog.</p>
<h1 id="applications-in-humanitarian-geoai">Applications in Humanitarian GeoAI</h1>
<p>Standardization efforts like STAC-MLM and TrainingDML-AI have
significant implications for humanitarian applications of geospatial AI.</p>
<p>First, <strong>data sharing for disaster response</strong> becomes more effective.
When multiple agencies contribute labeled datasets after an event,
consistent metadata enables others to quickly find, validate, and reuse
those datasets. TrainingDML-AI’s support for provenance and quality
metrics builds trust in shared resources, critical for decisions made
under crisis conditions (<a href="#ref-copernicus2023">Yue 2023</a>).</p>
<p>Second, <strong>pretrained models for humanitarian tasks</strong> become more
portable. Models for flood detection, crop failure prediction, or
building damage assessment can be described using STAC-MLM metadata,
making it easier for responders to identify and deploy relevant models
without building new ones from scratch (<a href="#ref-wherobots2023">Wherobots
2023</a>).</p>
<p>Third, <strong>collaboration and capacity building</strong> benefit greatly from
common standards. Volunteers, NGOs, and governments can work more easily
together when datasets and models are described in interoperable ways.
Standards lower technical barriers for cross-organizational efforts,
increasing the reach of humanitarian AI initiatives. Which can be also
seen in AI initiative called <a href="fAIr">fAIr</a> developed by Humanitarian
OpenStreetMap Team (HOTOSM)</p>
<p>Fourth, <strong>transparency and ethics</strong> are improved. Standardized model and
dataset descriptions enable users to assess the applicability and
limitations of AI systems, a critical concern in humanitarian contexts
where the consequences of model errors can be severe.</p>
<p>Finally, early examples such as Radiant Earth’s MLHub datasets and
SpaceNet challenges show that open, standardized datasets accelerate
innovation and improve humanitarian outcomes by enabling broader reuse
and benchmarking (<a href="#ref-radiant2019">Radiant Earth Foundation 2019</a>).</p>
<h1 id="conclusion">Conclusion</h1>
<p>The GeoAI field is undergoing a vital transformation through the
adoption of standards such as STAC-MLM and OGC TrainingDML-AI. These
initiatives address longstanding gaps in how geospatial datasets and
machine learning models are documented, discovered, and reused.
Open-source frameworks like Raster Vision and TerraTorch, and even
commercial platforms like Esri’s GeoAI toolbox, are increasingly
integrating standard practices, signaling a broader convergence across
sectors. Initiative like fAIr by HOTOSM which is trying to reduce the
complexity for wider community to use AI in disasters</p>
<p>Yet challenges remain. Although these standards provide robust
frameworks, real-world adoption is still limited, especially in
humanitarian contexts where ease of use is crucial. While developing
models and datasets can remain a technical task, deploying and
operationalizing them during emergencies must become significantly
easier. The lack of intuitive, user-friendly systems that leverage these
standards points to an urgent need for further research and tool
development.</p>
<p>As humanitarian challenges grow more complex and urgent, standardized,
discoverable, and easily deployable GeoAI models and datasets will
become foundational to effective response efforts. Continued
collaboration between open communities, standards bodies, researchers,
and commercial providers will be essential to realizing the full
potential of a FAIR, interoperable, and impactful GeoAI ecosystem.</p>
<h1 id="ai-use-disclaimer">AI Use Disclaimer</h1>
<p>This document was prepared with the assistance of AI-based tools,
including open source LLM LLAMA and OpenAI’s ChatGPT. AI tools were used
for structuring ideas, academic phrasing, and reference management based
on user-provided research.</p>
<h1 id="references">References</h1>
<hr>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-rastervision2020" class="csl-entry">
<p>Azavea. 2020. “Raster Vision: Deep Learning for Aerial and Satellite
Imagery.” 2020.
<a href="https://medium.com/azavea-engineering/raster-vision-deep-learning-for-aerial-and-satellite-imagery-d16b7e8f3f9b">https://medium.com/azavea-engineering/raster-vision-deep-learning-for-aerial-and-satellite-imagery-d16b7e8f3f9b</a>.</p>
</div>
<div id="ref-charette2024" class="csl-entry">
<p>Charette-Migneault, François, Yvan Bédard, and Patrice Vincent. 2024.
“Machine Learning Model Specification for Cataloging Spatio-Temporal
Models.” In <em>Proceedings of the 32nd ACM SIGSPATIAL International
Conference on Advances in Geographic Information Systems</em>.
<a href="https://doi.org/10.1145/3681769.3698586">https://doi.org/10.1145/3681769.3698586</a>.</p>
</div>
<div id="ref-esri2024" class="csl-entry">
<p>Esri. 2024. “GeoAI Toolbox in ArcGIS Pro.” 2024.
<a href="https://pro.arcgis.com/en/pro-app/latest/tool-reference/geoai/an-overview-of-the-geoai-toolbox.htm">https://pro.arcgis.com/en/pro-app/latest/tool-reference/geoai/an-overview-of-the-geoai-toolbox.htm</a>.</p>
</div>
<div id="ref-terratorch2024" class="csl-entry">
<p>Kumar, R., A. Suresh, and M. Srivastava. 2024. “TerraTorch: Benchmarking
and Fine-Tuning Geospatial Foundation Models.”
<a href="https://arxiv.org/abs/2404.00400">https://arxiv.org/abs/2404.00400</a>.</p>
</div>
<div id="ref-ogc2023" class="csl-entry">
<p>Open Geospatial Consortium. 2023. “OGC Training Data Markup Language for
Artificial Intelligence (TrainingDML-AI): Conceptual Model.” OGC
23-008r3. Open Geospatial Consortium.
<a href="https://docs.ogc.org/is/23-008r3/23-008r3.html">https://docs.ogc.org/is/23-008r3/23-008r3.html</a>.</p>
</div>
<div id="ref-radiant2019" class="csl-entry">
<p>Radiant Earth Foundation. 2019. “Radiant MLHub: An Open Registry for
Geospatial Machine Learning.” 2019.
<a href="https://registry.opendata.aws/radiant-mlhub/">https://registry.opendata.aws/radiant-mlhub/</a>.</p>
</div>
<div id="ref-radiant2021" class="csl-entry">
<p>———. 2021. “Geospatial Models in Radiant MLHub.” 2021.
<a href="https://medium.com/radiant-earth-insights/geospatial-models-now-available-in-radiant-mlhub-a41eb795d7d7">https://medium.com/radiant-earth-insights/geospatial-models-now-available-in-radiant-mlhub-a41eb795d7d7</a>.</p>
</div>
<div id="ref-wherobots2023" class="csl-entry">
<p>Wherobots. 2023. “Raster Inference and STAC-MLM: Making Geospatial
Models Discoverable.” 2023. <a href="https://wherobots.com/raster-inference/">https://wherobots.com/raster-inference/</a>.</p>
</div>
<div id="ref-copernicus2023" class="csl-entry">
<p>Yue, P. 2023. “Toward Standardization of Machine Learning Training
Datasets for Earth Observation.”
<a href="https://meetingorganizer.copernicus.org/EGU23/EGU23-11254.html">https://meetingorganizer.copernicus.org/EGU23/EGU23-11254.html</a>.</p>
</div>
</div> </div> <div class="mt-8 flex flex-wrap gap-x-3 text-sm sm:mt-12 sm:text-base"> <span class="font-semibold">Share:</span> <a class="text-primary transition duration-300 hover:text-slate-700" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fkshitijrajsharma.github.io%2Fblog%2Fstandards-in-geoai%2F&#38;text=Standards%20in%20Geo-AI%3A%20STAC-MLM%2C%20OGC%20TrainingDML-AI%2C%20and%20Related%20Initiatives" target="_blank" rel="noopener noreferer" aria-label="Share on X">X/Twitter</a> <a class="text-primary transition duration-300 hover:text-slate-700" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fkshitijrajsharma.github.io%2Fblog%2Fstandards-in-geoai%2F" target="_blank" rel="noopener noreferer" aria-label="Share on Facebook">Facebook</a> <a class="text-primary transition duration-300 hover:text-slate-700" href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fkshitijrajsharma.github.io%2Fblog%2Fstandards-in-geoai%2F&#38;summary=Standards%20in%20Geo-AI%3A%20STAC-MLM%2C%20OGC%20TrainingDML-AI%2C%20and%20Related%20Initiatives" target="_blank" rel="noopener noreferer" aria-label="Share on LinkedIn">LinkedIn</a> </div> </div> </article> <div class="max-w-3xl mx-auto mb-12 sm:mb-16"> <h2 class="mb-8 text-sm uppercase tracking-wider text-slate-900 after:content-[''] after:block after:w-16 after:h-px after:bg-primary after:mt-4 sm:mb-12">
Read Next
</h2>  <article class="mb-12 flex flex-col gap-x-8 gap-y-6 sm:flex-row"> <header class="grow"> <div class="mb-2 text-sm uppercase tracking-wider text-slate-500"> <time datetime="2024-12-12T23:00:00.000Z"> December 13, 2024 </time>  </div> <h3 class="text-2xl font-bold text-slate-900"> <a href="/blog/radar-image-tutorial/">Working with RADAR : Case Study in Jajarkot EarthQuake</a> </h3> </header> <figure class="shrink-0 sm:w-40"> <a href="/blog/radar-image-tutorial/"> <img class="w-full rounded-md" src="https://github.com/user-attachments/assets/3ec2e1db-36f5-4838-8242-d445a11f909e" loading="lazy" decoding="async" alt="Earthquake Displacement"> </a> </figure> </article> </div> <section class="my-12 sm:my-16"><div class="w-full max-w-3xl mx-auto px-4 py-8 flex flex-col items-center bg-slate-50 border border-slate-200 rounded-md text-center sm:px-12 sm:py-14"><form action="#" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="w-full max-w-xl flex flex-col gap-2.5 sm:flex-row" target="_blank"><label for="mce-EMAIL" class="sr-only">
Email Address
</label><input type="text" name="search" id="mce-EMAIL" class="w-full h-11 px-5 py-2 text-slate-700 bg-white border border-slate-200 rounded-full placeholder:text-slate-500 focus:outline-none" required="" value="" placeholder="Your text"><button type="submit" name="subscribe" id="mc-embedded-subscribe" class="w-full px-8 py-2.5 inline-flex gap-1.5 justify-center items-center font-semibold bg-primary text-white rounded-full transition duration-300 hover:shadow-button sm:w-auto">
Search
</button></form></div></section> </main>  <footer class="px-4 py-8 sm:px-8 sm:py-12"> <div class="w-full max-w-3xl mx-auto mb-6 flex flex-wrap gap-x-6 gap-y-1 justify-center"> <a class="text-slate-700 transition hover:text-slate-500" href="/about"> About </a><a class="text-slate-700 transition hover:text-slate-500" href="/terms"> Terms of Service </a><a class="text-slate-700 transition hover:text-slate-500" href="/contact"> Contact </a> </div> <div class="w-full max-w-3xl mx-auto mb-6 flex flex-wrap gap-x-4 gap-y-3 justify-center"> <a class="w-11 h-11 inline-flex items-center justify-center rounded-full transition duration-300 bg-slate-50 text-slate-700 hover:bg-primary hover:text-white hover:shadow-button" href="https://github.com/kshitijrajsharma" target="_blank" rel="noopener noreferer" aria-label="Go to GitHub repo"> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="w-5 h-5 fill-current overflow-visible"> <path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path> </svg> </a><a class="w-11 h-11 inline-flex items-center justify-center rounded-full transition duration-300 bg-slate-50 text-slate-700 hover:bg-primary hover:text-white hover:shadow-button" href="https://linkedin.com/in/kshitijrajsharma" target="_blank" rel="noopener noreferer" aria-label="Follow on Linkedin"> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="w-5 h-5 fill-current overflow-visible"> <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path> </svg> </a><a class="w-11 h-11 inline-flex items-center justify-center rounded-full transition duration-300 bg-slate-50 text-slate-700 hover:bg-primary hover:text-white hover:shadow-button" href="https://mastodon.social/@kshitijrajsharma" target="_blank" rel="noopener noreferer" aria-label="Follow on Mastodon"> <svg width="24" height="24" viewBox="0 0 74 79" fill="black" xmlns="http://www.w3.org/2000/svg" class="w-5 h-5 fill-current overflow-visible"> <path d="M73.7014 17.4323C72.5616 9.05152 65.1774 2.4469 56.424 1.1671C54.9472 0.950843 49.3518 0.163818 36.3901 0.163818H36.2933C23.3281 0.163818 20.5465 0.950843 19.0697 1.1671C10.56 2.41145 2.78877 8.34604 0.903306 16.826C-0.00357854 21.0022 -0.100361 25.6322 0.068112 29.8793C0.308275 35.9699 0.354874 42.0498 0.91406 48.1156C1.30064 52.1448 1.97502 56.1419 2.93215 60.0769C4.72441 67.3445 11.9795 73.3925 19.0876 75.86C26.6979 78.4332 34.8821 78.8603 42.724 77.0937C43.5866 76.8952 44.4398 76.6647 45.2833 76.4024C47.1867 75.8033 49.4199 75.1332 51.0616 73.9562C51.0841 73.9397 51.1026 73.9184 51.1156 73.8938C51.1286 73.8693 51.1359 73.8421 51.1368 73.8144V67.9366C51.1364 67.9107 51.1302 67.8852 51.1186 67.862C51.1069 67.8388 51.0902 67.8184 51.0695 67.8025C51.0489 67.7865 51.0249 67.7753 50.9994 67.7696C50.9738 67.764 50.9473 67.7641 50.9218 67.7699C45.8976 68.9569 40.7491 69.5519 35.5836 69.5425C26.694 69.5425 24.3031 65.3699 23.6184 63.6327C23.0681 62.1314 22.7186 60.5654 22.5789 58.9744C22.5775 58.9477 22.5825 58.921 22.5934 58.8965C22.6043 58.8721 22.621 58.8505 22.6419 58.8336C22.6629 58.8167 22.6876 58.8049 22.714 58.7992C22.7404 58.7934 22.7678 58.794 22.794 58.8007C27.7345 59.9796 32.799 60.5746 37.8813 60.5733C39.1036 60.5733 40.3223 60.5733 41.5447 60.5414C46.6562 60.3996 52.0437 60.1408 57.0728 59.1694C57.1983 59.1446 57.3237 59.1233 57.4313 59.0914C65.3638 57.5847 72.9128 52.8555 73.6799 40.8799C73.7086 40.4084 73.7803 35.9415 73.7803 35.4523C73.7839 33.7896 74.3216 23.6576 73.7014 17.4323ZM61.4925 47.3144H53.1514V27.107C53.1514 22.8528 51.3591 20.6832 47.7136 20.6832C43.7061 20.6832 41.6988 23.2499 41.6988 28.3194V39.3803H33.4078V28.3194C33.4078 23.2499 31.3969 20.6832 27.3894 20.6832C23.7654 20.6832 21.9552 22.8528 21.9516 27.107V47.3144H13.6176V26.4937C13.6176 22.2395 14.7157 18.8598 16.9118 16.3545C19.1772 13.8552 22.1488 12.5719 25.8373 12.5719C30.1064 12.5719 33.3325 14.1955 35.4832 17.4394L37.5587 20.8853L39.6377 17.4394C41.7884 14.1955 45.0145 12.5719 49.2765 12.5719C52.9614 12.5719 55.9329 13.8552 58.2055 16.3545C60.4017 18.8574 61.4997 22.2371 61.4997 26.4937L61.4925 47.3144Z" fill="inherit"></path> </svg> </a><a class="w-11 h-11 inline-flex items-center justify-center rounded-full transition duration-300 bg-slate-50 text-slate-700 hover:bg-primary hover:text-white hover:shadow-button" href="https://dev.to/krschap" target="_blank" rel="noopener noreferer" aria-label="Follow on Dev"> <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="w-5 h-5 fill-current overflow-visible"> <path d="M7.42 10.05c-.18-.16-.46-.23-.84-.23H6l.02 2.44.04 2.45.56-.02c.41 0 .63-.07.83-.26.24-.24.26-.36.26-2.2 0-1.91-.02-1.96-.29-2.18zM0 4.94v14.12h24V4.94H0zM8.56 15.3c-.44.58-1.06.77-2.53.77H4.71V8.53h1.4c1.67 0 2.16.18 2.6.9.27.43.29.6.32 2.57.05 2.23-.02 2.73-.47 3.3zm5.09-5.47h-2.47v1.77h1.52v1.28l-.72.04-.75.03v1.77l1.22.03 1.2.04v1.28h-1.6c-1.53 0-1.6-.01-1.87-.3l-.3-.28v-3.16c0-3.02.01-3.18.25-3.48.23-.31.25-.31 1.88-.31h1.64v1.3zm4.68 5.45c-.17.43-.64.79-1 .79-.18 0-.45-.15-.67-.39-.32-.32-.45-.63-.82-2.08l-.9-3.39-.45-1.67h.76c.4 0 .75.02.75.05 0 .06 1.16 4.54 1.26 4.83.04.15.32-.7.73-2.3l.66-2.52.74-.04c.4-.02.73 0 .73.04 0 .14-1.67 6.38-1.8 6.68z"></path> </svg> </a> </div> <p class="text-center text-xs uppercase tracking-wider text-slate-700">
&copy; 2025&nbsp;<a class="text-primary underline hover:no-underline" href="/">Horizournal</a>. All rights reserved.
</p> </footer> </div> </body></html>